{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902495cf",
   "metadata": {},
   "source": [
    "scan wa page\n",
    "--> request page's html\n",
    "--> save the necessary part of the page into a txt file\n",
    "--> scan through job links on that page\n",
    "--> get the nth link -> save to a list -> request -> save file -> process -> add to df\n",
    "--> k00th link -> save to csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cạnh tranh gần nhất:\n",
    "\n",
    "https://pantio.vn/\n",
    "\n",
    "https://ivymoda.com/\n",
    "\n",
    "https://elise.vn/\n",
    "\n",
    "(1 - 3 năm)\n",
    "\n",
    "\n",
    "https://sixdo.vn/\n",
    "\n",
    "(lâu dài)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8249ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urlparse, unquote\n",
    "# os.environ['PATH'] += r\"E:\\Installers\\chromedriver_win32\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResult(scheme='https', netloc='elise.vn', path='/thoi-trang-nu', params='', query='', fragment='')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_site_url = 'https://elise.vn/thoi-trang-nu'\n",
    "urlparse(main_site_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get numbered pages' url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\COLLEGE\\\\PROJECT\\\\Thay Nhan\\\\INTERN\\\\Crawling\\\\crawling_code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pages(main_site_url, start_page, end_page, url_format):\n",
    "    all_pages_list = []\n",
    "    all_pages_list += [main_site_url]\n",
    "    \n",
    "    for i in range(start_page, end_page+1):\n",
    "        url = url_format.format(i)\n",
    "        all_pages_list.append(url)\n",
    "    \n",
    "    parsed_url = urlparse(main_site_url)\n",
    "    domain = parsed_url.netloc\n",
    "    \n",
    "    file_path = os.path.join(os.path.dirname(path), 'crawling_materials')\n",
    "    # os.path.dirname(path) là để lùi lại thư mục Crawling\n",
    "    pages_links_file_name = os.path.join(file_path, f'numbered_pages_{domain}.txt')\n",
    "    \n",
    "    \n",
    "    with open(pages_links_file_name, 'w', encoding = 'utf-8') as f:\n",
    "        for page in all_pages_list:\n",
    "            f.write(f'{page}\\n')\n",
    "    \n",
    "    return pages_links_file_name # This file stores all the links to the pagination pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all products' links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_links(pagination_pages):\n",
    "    wanted_links = []\n",
    "    with open(pagination_pages, 'r') as f:\n",
    "#         lines = f.readlines()[:2]  # Read only the first two lines to test\n",
    "#         for line in lines:\n",
    "        for line in f:\n",
    "            page_url = line.strip()\n",
    "            response = requests.get(page_url)\n",
    "            html_content = response.content.decode()\n",
    "            soup = BeautifulSoup(html_content, \"html5lib\")\n",
    "\n",
    "            # FOR LATINA.VN\n",
    "            if 'latina' in page_url:\n",
    "                product_names = soup.find_all('div', class_='product-name')\n",
    "\n",
    "                for product in product_names:\n",
    "                    link = product.find('a', class_='link')['href']\n",
    "                    hyperlink = \"https://latina.vn\" + link\n",
    "                    wanted_links += [hyperlink]\n",
    "            elif 'elise' in page_url:\n",
    "                product\n",
    "                \n",
    "            print(f'Finished: {line}.')\n",
    "            clear_output(wait=True)\n",
    "    \n",
    "    match = re.search(r'numbered_pages_(.*?)\\.txt', pagination_pages)\n",
    "    if match:\n",
    "        domain = match.group(1)\n",
    "        file_name = f'wanted_links_{domain}.txt'\n",
    "        \n",
    "    file_path = os.path.join(os.path.dirname(path), 'crawling_materials')\n",
    "    file_wanted_links = os.path.join(file_path, file_name)\n",
    "    \n",
    "    with open(file_wanted_links, 'w', encoding='utf-8') as f:\n",
    "        for link in wanted_links:\n",
    "            f.write(f'{link}\\n')\n",
    "\n",
    "    return file_wanted_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'latina'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://latina.vn/collections/all?q=filter=(!(collectionid:product=0))&page=19&view=grid'\n",
    "parsed_url = urlparse(url)\n",
    "\n",
    "# Extract the domain_folder from the netloc\n",
    "domain_folder = parsed_url.netloc.split('.')[0] if parsed_url.netloc.count('.') > 0 else 'other'\n",
    "domain_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'set-bo-thiet-ke-cao-cap-ms4236'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://latina.vn/products/set-bo-thiet-ke-cao-cap-ms4236'\n",
    "parsed_url = urlparse(url)\n",
    "\n",
    "if parsed_url.netloc == 'latina.vn':\n",
    "        path_parts = parsed_url.path.split('/')\n",
    "        product_name = path_parts[-1]\n",
    "product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85db0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse, unquote\n",
    "from datetime import datetime\n",
    "\n",
    "def get_next_subfolder(day_folder):\n",
    "    file_size = 1000\n",
    "    subfolder_id = file_size  \n",
    "    while True:\n",
    "        subfolder_path = os.path.join(day_folder, str(subfolder_id))\n",
    "        if not os.path.exists(subfolder_path) or len(os.listdir(subfolder_path)) < file_size:\n",
    "            return subfolder_path\n",
    "        subfolder_id += file_size\n",
    "\n",
    "def file_naming(url, output_folder):\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    # Extract the domain_folder from the netloc\n",
    "    # FOR LATINA\n",
    "    domain_folder = parsed_url.netloc.split('.')[0] if parsed_url.netloc.count('.') > 0 else 'other'\n",
    "\n",
    "    # Get the current date and time\n",
    "    current_datetime = datetime.now()\n",
    "    \n",
    "    # Create subfolders based on year, month, and day\n",
    "    page_folder = os.path.join(output_folder, domain_folder)\n",
    "    year_folder = os.path.join(page_folder, str(current_datetime.year))\n",
    "    month_folder = os.path.join(year_folder, str(current_datetime.month))\n",
    "    day_folder = os.path.join(month_folder, str(current_datetime.day))\n",
    "    \n",
    "    \n",
    "    subfolder_path = get_next_subfolder(day_folder)\n",
    "    os.makedirs(subfolder_path, exist_ok=True)  \n",
    "\n",
    "    if parsed_url.netloc == 'latina.vn':\n",
    "        if parsed_url.netloc == 'latina.vn':\n",
    "            path_parts = parsed_url.path.split('/')\n",
    "            product_name = path_parts[-1]\n",
    "\n",
    "    else:\n",
    "        product_name = None\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        file_id = f\"{i:05d}\"\n",
    "        file_name = f\"[{current_datetime.strftime('%Y%m%d %H%M%S')}]_{file_id}_{product_name}.txt\"\n",
    "        file_path = os.path.join(subfolder_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            return file_path\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f9d4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "def saving_numbered_pages(url, output_folder=r'E:\\Test'):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # FOR LATINA\n",
    "    if '&' not in url:\n",
    "        pagenum = 'page1'\n",
    "    else:\n",
    "        # FOR LANINA\n",
    "        pagenum = url.split('&')[-2]\n",
    "        pagenum = re.sub(r'(?u)[^-\\w.]', '', pagenum)\n",
    "    \n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    # Extract the domain_folder from the netloc\n",
    "    # FOR LATINA\n",
    "    domain_folder = parsed_url.netloc.split('.')[0] if parsed_url.netloc.count('.') > 0 else 'other'\n",
    "\n",
    "\n",
    "    # Get the current date and time\n",
    "    current_datetime = datetime.now()\n",
    "    \n",
    "    # Create subfolders based on year, month, and day\n",
    "    page_folder = os.path.join(output_folder, domain_folder)\n",
    "    year_folder = os.path.join(page_folder, str(current_datetime.year))\n",
    "    month_folder = os.path.join(year_folder, str(current_datetime.month))\n",
    "    day_folder = os.path.join(month_folder, str(current_datetime.day))\n",
    "    numbered_pages_folder = os.path.join(day_folder, 'numbered pages')\n",
    "    os.makedirs(numbered_pages_folder, exist_ok=True)  \n",
    "    \n",
    "    i = 1\n",
    "    while True:\n",
    "        file_id = f\"{i:05d}\"\n",
    "        file_name = f\"[{current_datetime.strftime('%Y%m%d %H%M%S')}]_{domain_folder}_{file_id}_{pagenum}.txt\"\n",
    "        file_path = os.path.join(numbered_pages_folder, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            break\n",
    "        i += 1\n",
    "    \n",
    "    # FOR LATINA\n",
    "    if 'latina' in url:\n",
    "        needed_div = soup.find('div', class_='category-products products')\n",
    "        if needed_div:\n",
    "            div_content = needed_div.prettify()\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(div_content)\n",
    "    \n",
    "    # FOR CAREER LINK\n",
    "    elif 'careerlink' in url:\n",
    "        needed_div = soup.find('ul', class_='list-group')\n",
    "        if needed_div:\n",
    "            div_content = needed_div.prettify()\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(div_content)\n",
    "    \n",
    "    # FOR VNWORKS\n",
    "    elif 'vietnamworks' in url:\n",
    "        needed_div = soup.find('div', class_='col-lg-9')\n",
    "        if needed_div:\n",
    "            div_content = needed_div.prettify()\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(div_content)\n",
    "    \n",
    "    # FOR HOTEL\n",
    "    elif 'hotel' in url:\n",
    "        needed_div = soup.find('div', class_='job-contain')\n",
    "        if needed_div:\n",
    "            div_content = needed_div.prettify()\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(div_content)\n",
    "            \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://latina.vn/collections/all?q=filter=(!(collectionid:product=0))&page=20&view=grid\n",
      "Overall progress: 5.0%\n"
     ]
    }
   ],
   "source": [
    "# with open(latina_pages, 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "#     overall_sum = len(lines)\n",
    "#     overall_count = 1\n",
    "#     for line in lines:\n",
    "#         page_url = line.strip()\n",
    "#         print(page_url)\n",
    "#         numbered_page_path = saving_numbered_pages(page_url, 'E:\\COLLEGE\\PROJECT\\Thay Nhan\\INTERN\\Crawling\\data')\n",
    "        \n",
    "#         i = 0\n",
    "#         overall_progress = round((overall_count / overall_sum)*100, 2)\n",
    "#         print(f'Overall progress: {overall_progress}%')\n",
    "#         clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d9e40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_detail_file(url, output_folder):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # FOR LATINA\n",
    "    if 'latina' in url:\n",
    "        needed_div = soup.find('section', class_='product detail-main')\n",
    "    # FOR CAREER LINK\n",
    "    if 'careerlink' in url:\n",
    "        needed_div = soup.find('div', class_='card border-0 font-nunitosans px-4')\n",
    "        \n",
    "    # FOR VNWORKS\n",
    "    if 'vietnamworks' in url:\n",
    "        needed_div = soup.find('div', class_='page-foreground')\n",
    "        \n",
    "    # FOR HOTEL JOB\n",
    "    if 'hotel' in url:\n",
    "        needed_div = soup.find('section', class_='py-lg-4')\n",
    "        \n",
    "    if needed_div:\n",
    "        div_content = needed_div.prettify()\n",
    "        file_path = file_naming(url, output_folder)\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(div_content)\n",
    "        return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b659799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse, unquote\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def save_csv_file(df, batch_number, domain, output_folder=r\"E:\\test\"):\n",
    "    \n",
    "    current_datetime = datetime.now()\n",
    "    \n",
    "    # Create subfolders based on year, month, and day\n",
    "    year_folder = os.path.join(output_folder, str(current_datetime.year))\n",
    "    month_folder = os.path.join(year_folder, str(current_datetime.month))\n",
    "    day_folder = os.path.join(month_folder, str(current_datetime.day))\n",
    "    csv_folder = os.path.join(day_folder, domain)\n",
    "    \n",
    "    \n",
    "    os.makedirs(csv_folder, exist_ok=True)  \n",
    "\n",
    "    \n",
    "    i = 1\n",
    "    while True:\n",
    "        file_id = f\"{i:05d}\"\n",
    "        file_name = f\"[{current_datetime.strftime('%Y%m%d %H%M%S')}]_{file_id}_batch{batch_number}.csv\"\n",
    "        file_path = os.path.join(csv_folder, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            df.to_csv(file_path, index=False)\n",
    "            return\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03177377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_ids(url, output_file='ids.txt'):\n",
    "    id = None\n",
    "\n",
    "    if 'latina' in url:\n",
    "        id = 'latin_' + url.split('-')[-1]\n",
    "    elif 'careerlink' in url:\n",
    "        id = 'careerlink_' + url.split('?')[0].split('/')[-1]\n",
    "    elif 'vietnamworks' in url:\n",
    "        id = 'vnworks_' + url.split('&')[-2].split('=')[-1]\n",
    "    elif 'hotel' in url:\n",
    "        id = 'hoteljob_' + url.split('/')[-1].split('-')[0]\n",
    "\n",
    "    if id is not None:\n",
    "        try:\n",
    "            with open(output_file, 'r') as file:\n",
    "                existing_ids = [line.strip() for line in file]\n",
    "                if id in existing_ids:\n",
    "                    return False\n",
    "                else:\n",
    "                    with open(output_file, 'a') as file:\n",
    "                        file.write(id + '\\n')\n",
    "                        return True\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'VNWork_WantedLinks_Backup.txt', 'r', encoding='utf-8') as f:\n",
    "    urls = f.readlines()\n",
    "for url in urls:\n",
    "    extract_and_save_ids(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3e203",
   "metadata": {},
   "source": [
    "# Career Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71a20c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(soup):\n",
    "    sum_cols = soup.find_all('div', class_='col-6')\n",
    "\n",
    "\n",
    "    test_dict = {}\n",
    "    for col in sum_cols:\n",
    "        sum_boxes = col.find_all('div', class_='d-flex')\n",
    "        for box in sum_boxes:\n",
    "            key = box.find('div', class_='summary-label')\n",
    "            if key != None:\n",
    "                key = key.text.strip()\n",
    "                val = box.find('div', class_='font-weight-bolder')\n",
    "                val = val.text.strip()\n",
    "                test_dict[key] = val\n",
    "\n",
    "        other_sum_boxes = col.find_all('div', class_='row')\n",
    "        for box in other_sum_boxes:\n",
    "            key = box.find('div', class_='summary-label')\n",
    "            if key != None:\n",
    "                key = key.text.strip()\n",
    "                val_list = box.find_all('span')\n",
    "                val_list = [span.text.strip() for span in val_list]\n",
    "                test_dict[key] = val_list\n",
    "\n",
    "    return test_dict\n",
    "\n",
    "def get_info(soup):\n",
    "    job_info_elements = soup.find('div', id='section-job-contact-information')\n",
    "    info_boxes = job_info_elements.find_all('li', class_='d-flex')\n",
    "\n",
    "    contact_dict = {'Contact': None, 'Location': None, 'Notes': None}\n",
    "    for box in info_boxes:\n",
    "        key_contact = box.find('i', class_='cli-contact-with')\n",
    "        key_loc = box.find('i', class_='cli-location')\n",
    "        key_notes = box.find('i', class_='cli-note')\n",
    "\n",
    "        if key_contact != None:\n",
    "            val_list = []\n",
    "            label = box.find('span', class_='label').text.strip()\n",
    "            person_name = box.find('span', class_='person-name').text.strip()\n",
    "            val_list.append(label)\n",
    "            val_list.append(person_name)\n",
    "            contact_dict['Contact'] = val_list\n",
    "\n",
    "        if key_loc != None:\n",
    "            contact_loc = box.find('span', class_='align-seft-center')\n",
    "            contact_loc_list = contact_loc.find_all('span')\n",
    "            contact_loc_list = [span.text.strip() for span in contact_loc_list]\n",
    "            contact_dict['Location'] = contact_loc_list\n",
    "\n",
    "        if key_notes != None:\n",
    "            contact_note_list = box.find_all('em')\n",
    "            for i in range(len(contact_note_list)):\n",
    "                contact_note_list[i] = contact_note_list[i].text.strip()\n",
    "            contact_dict['Notes'] = contact_note_list\n",
    "\n",
    "    return contact_dict\n",
    "\n",
    "def get_keywords(soup):\n",
    "    job_keyword_elements = soup.find('div', class_='tags-container')\n",
    "    all_tag_chips = job_keyword_elements.find_all('a', class_='chip')\n",
    "    keywords = []\n",
    "\n",
    "    for tag_chip in all_tag_chips:\n",
    "        chip_list = [tag_chip.text.strip(), \"https://www.careerlink.vn/\"+tag_chip.get(\"href\")]\n",
    "        keywords.append(chip_list)\n",
    "        \n",
    "    return keywords\n",
    "\n",
    "def get_overview(soup):\n",
    "    job_overview = soup.find('div', class_='job-overview')\n",
    "    overview_boxes = job_overview.find_all('div', class_='d-flex')\n",
    "\n",
    "    location_list = []  # Initialize the location list\n",
    "\n",
    "    for box in overview_boxes:\n",
    "        location = box.find('i', class_='cli-map-pin-line')\n",
    "        salary = box.find('i', class_='cli-currency-circle-dollar')\n",
    "        experience = box.find('i', class_='cli-suitcase-simple')\n",
    "        post_date = box.find('i', class_='cli-calendar')\n",
    "\n",
    "        if location != None:\n",
    "            loc_spans = box.find_all('span', class_='mr-1')\n",
    "            location_list = [span.text.strip() for span in loc_spans]\n",
    "            city = box.find('a').text.strip()\n",
    "            location_list += [city]\n",
    "\n",
    "        if salary != None:\n",
    "            job_salary = box.find('span').text.strip()\n",
    "\n",
    "        if experience != None:\n",
    "            job_exp = box.find('span').text.strip()\n",
    "\n",
    "        if post_date != None:\n",
    "            date_span = box.find('span', class_='d-none d-md-block mr-1')\n",
    "            job_post_date = date_span.next_sibling.strip() if date_span else None\n",
    "\n",
    "    return location_list, job_salary, job_exp, job_post_date\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "def save_to_df(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html5lib\")\n",
    "    \n",
    "    job_title = soup.find('h1', class_='job-title').text.strip()\n",
    "    \n",
    "    location, job_salary, job_exp, job_post_date = get_overview(soup)\n",
    "    \n",
    "    org_info = soup.find('p', class_='org-name')\n",
    "    a_tag = org_info.find('a')\n",
    "    org_name = a_tag.span.text.strip()\n",
    "    org_link = \"https://www.careerlink.vn\" + a_tag.get(\"href\")\n",
    "    \n",
    "    job_desc_elements = soup.find('div', id='section-job-description').find_all('p')\n",
    "    job_desc = [p.text.strip() for p in job_desc_elements]\n",
    "    \n",
    "    job_benefits_elements = soup.find('div', id='section-job-benefits')\n",
    "    if job_benefits_elements:\n",
    "        job_benefits_elements = job_benefits_elements.find_all('span')\n",
    "        job_benefits = [span.text.strip() for span in job_benefits_elements]\n",
    "    else:\n",
    "        job_benefits = None\n",
    "\n",
    "    \n",
    "    job_skills_elements = soup.find('div', id='section-job-skills').find_all('p')\n",
    "    job_skills = [p.text.strip() for p in job_skills_elements]\n",
    "    \n",
    "    job_summary = get_summary(soup)\n",
    "    contact_info = get_info(soup)\n",
    "    keywords = get_keywords(soup)\n",
    "    \n",
    "    logo_url = soup.find('img', class_='company-img').get('src')\n",
    "\n",
    "    job_data = {\n",
    "        'Job Title': job_title,\n",
    "        'Location': location, #City = location[-1]\n",
    "        'Salary': job_salary,\n",
    "        'Experience': job_exp,\n",
    "        'Company': org_name,\n",
    "        'Company Link': org_link,\n",
    "        'Description': job_desc,\n",
    "        'Benefits': job_benefits,\n",
    "        'Required Skills': job_skills,\n",
    "        'Job Summary': job_summary,\n",
    "        'Contact': contact_info,\n",
    "        'Keywords': keywords,\n",
    "        'Post date': job_post_date,\n",
    "        'Logo URL': logo_url,\n",
    "        'Field': None,\n",
    "        \n",
    "    }\n",
    "\n",
    "    jobs_data.append(job_data)\n",
    "    print(f'Finished {file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e81028",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551997f6",
   "metadata": {},
   "source": [
    "## Get new links, new files, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9bb38292",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_data = []\n",
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d24aab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall progress: 0.5%\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\COLLEGE\\\\PROJECT\\\\Thay Nhan\\\\INTERN\\\\Crawling\\\\data\\\\latina\\\\2024\\\\2\\\\23\\\\numbered pages\\\\[20240223 113821]_latina_00001_page1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\COLLEGE\\PROJECT\\Thay Nhan\\INTERN\\Crawling\\crawling_code\\crawling_fashion_sites.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/COLLEGE/PROJECT/Thay%20Nhan/INTERN/Crawling/crawling_code/crawling_fashion_sites.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mOverall progress: \u001b[39m\u001b[39m{\u001b[39;00moverall_progress\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/COLLEGE/PROJECT/Thay%20Nhan/INTERN/Crawling/crawling_code/crawling_fashion_sites.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/COLLEGE/PROJECT/Thay%20Nhan/INTERN/Crawling/crawling_code/crawling_fashion_sites.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m             \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(numbered_page_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/COLLEGE/PROJECT/Thay%20Nhan/INTERN/Crawling/crawling_code/crawling_fashion_sites.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                 html_content \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/COLLEGE/PROJECT/Thay%20Nhan/INTERN/Crawling/crawling_code/crawling_fashion_sites.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#             job_links = soup.find_all('a', class_='job-link')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/COLLEGE/PROJECT/Thay%20Nhan/INTERN/Crawling/crawling_code/crawling_fashion_sites.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#             links = []\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/COLLEGE/PROJECT/Thay%20Nhan/INTERN/Crawling/crawling_code/crawling_fashion_sites.ipynb#X20sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m#             for job in job_links:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/COLLEGE/PROJECT/Thay%20Nhan/INTERN/Crawling/crawling_code/crawling_fashion_sites.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m#                 job_url = 'https://www.careerlink.vn/' + job.get('href')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/COLLEGE/PROJECT/Thay%20Nhan/INTERN/Crawling/crawling_code/crawling_fashion_sites.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#                 links += [job_url]    \u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\COLLEGE\\\\PROJECT\\\\Thay Nhan\\\\INTERN\\\\Crawling\\\\data\\\\latina\\\\2024\\\\2\\\\23\\\\numbered pages\\\\[20240223 113821]_latina_00001_page1.txt'"
     ]
    }
   ],
   "source": [
    "website_pages_file = latina_pages\n",
    "output_folder = r'E:\\COLLEGE\\PROJECT\\Thay Nhan\\INTERN\\Crawling\\data'\n",
    "\n",
    "jobs_data = []\n",
    "\n",
    "batch_size = 100\n",
    "start = 0\n",
    "\n",
    "with open(website_pages_file, 'r') as f:\n",
    "    lines = f.readlines()[:200]\n",
    "    overall_sum = len(lines)\n",
    "    overall_count = 1\n",
    "    for line in lines:\n",
    "        page_url = line.strip()\n",
    "        numbered_page_path = saving_numbered_pages(page_url, output_folder)\n",
    "        \n",
    "        i = 0\n",
    "        overall_progress = round((overall_count / overall_sum)*100, 2)\n",
    "        print(f'Overall progress: {overall_progress}%')\n",
    "        while True:\n",
    "            \n",
    "            with open(numbered_page_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "                \n",
    "#             job_links = soup.find_all('a', class_='job-link')\n",
    "#             links = []\n",
    "#             for job in job_links:\n",
    "#                 job_url = 'https://www.careerlink.vn/' + job.get('href')\n",
    "#                 links += [job_url]    \n",
    "\n",
    "            links = re.findall(r'<a\\s+class=\"job-link clickable-outside\"\\s+href=\"([^\"]+)\"\\s+title=\"[^\"]*\">', html_content)\n",
    "            if i < len(links):\n",
    "                link = links[i]\n",
    "                hyperlink = \"https://www.careerlink.vn\" + link\n",
    "                \n",
    "                if extract_and_save_ids(hyperlink) == True:\n",
    "                    file_path = saving_detail_file(hyperlink, output_folder)\n",
    "                    \n",
    "                    try:\n",
    "                        # save_to_df(file_path)\n",
    "                        print('Done')\n",
    "                        if len(jobs_data) % batch_size == 0:\n",
    "                            batch_number = len(jobs_data) // batch_size\n",
    "                            batch_df = pd.DataFrame(jobs_data[start : start+batch_size])\n",
    "                            print('hhihi')\n",
    "                            start += batch_size\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f'Error processing {file_path}: {str(e)}')\n",
    "                        errors.append(file_path)\n",
    "                        \n",
    "                        output_file = 'CareerLink_Errors.txt'\n",
    "                        with open(output_file, 'a') as output_file:\n",
    "                            output_file.write(hyperlink + '\\n')\n",
    "                else:\n",
    "                    print('ALR EXISTED, skipping....')        \n",
    "                        \n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "                \n",
    "        overall_count += 1\n",
    "        clear_output(wait=True)\n",
    "\n",
    "\n",
    "# Save any remaining records to a final CSV file\n",
    "if len(jobs_data) % batch_size != 0:\n",
    "    saved_items_amt = batch_size * batch_number\n",
    "    batch_df = pd.DataFrame(jobs_data[saved_items_amt:])\n",
    "    print('hihii')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d02044",
   "metadata": {},
   "source": [
    "## Re-iterate through all available html files as a whole (for re-running case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b745552a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29806"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_folder = r'E:\\Crawling Result\\careerlink'  # Replace with your folder path\n",
    "\n",
    "# Function to list all subfolders recursively, excluding 'numbered pages'\n",
    "def list_subfolders(root_path):\n",
    "    subfolders = [x[0] for x in os.walk(root_path) if 'numbered pages' not in x[0]]\n",
    "    return subfolders\n",
    "\n",
    "def find_html_files(subfolders):\n",
    "    html_files = []\n",
    "    for subfolder in subfolders:\n",
    "        for file in os.listdir(subfolder):\n",
    "            if file.lower().endswith('.txt'):\n",
    "                html_files.append(os.path.join(subfolder, file))\n",
    "    return html_files\n",
    "\n",
    "subfolders = list_subfolders(root_folder)\n",
    "html_files = find_html_files(subfolders)\n",
    "\n",
    "len(html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66adb797",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'careerlink'\n",
    "website_pages_file = careerlink_pages\n",
    "output_folder = r'E:\\Crawling Result'\n",
    "\n",
    "jobs_data = []\n",
    "\n",
    "batch_size = 100\n",
    "start = 0\n",
    "overall_sum = len(html_files)\n",
    "overall_count = 1\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    if i < len(html_files):\n",
    "        file_path = html_files[i]\n",
    "        \n",
    "        overall_progress = round((overall_count / overall_sum)*100, 2)\n",
    "        print(f'Overall progress: {overall_progress}%')\n",
    "\n",
    "        try:\n",
    "            save_to_df(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {file_path}: {str(e)}')\n",
    "            errors.append(file_path)\n",
    "\n",
    "            output_file = 'CareerLink_Errors.txt'\n",
    "            with open(output_file, 'a') as output_file:\n",
    "                output_file.write(hyperlink + '\\n')\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    if len(jobs_data) % batch_size == 0:\n",
    "        batch_number = len(jobs_data) // batch_size\n",
    "        batch_df = pd.DataFrame(jobs_data[start : start+batch_size])\n",
    "        save_csv_file(batch_df, batch_number, domain, output_folder)\n",
    "        start += batch_size\n",
    "\n",
    "    overall_count += 1\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "# Save any remaining records to a final CSV file\n",
    "if len(jobs_data) % batch_size != 0:\n",
    "    saved_items_amt = batch_size * batch_number\n",
    "    batch_df = pd.DataFrame(jobs_data[saved_items_amt:])\n",
    "    save_csv_file(batch_df, batch_number + 1, domain, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73043624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(jobs_data)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac2b6d3",
   "metadata": {},
   "source": [
    "# VietNam Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be65fd",
   "metadata": {},
   "source": [
    "## Detail View Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac0592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(soup):\n",
    "    job_keyword_elements = soup.find('div', class_='job-tags')\n",
    "    all_tag_chips = job_keyword_elements.find_all('a')\n",
    "    keywords = []\n",
    "\n",
    "    for tag_chip in all_tag_chips:\n",
    "        tag_name = tag_chip.find('span', class_='tag-name').text.strip()\n",
    "        chip_list = [tag_name, tag_chip.get('href')]\n",
    "        keywords.append(chip_list)\n",
    "        \n",
    "    return keywords\n",
    "\n",
    "def get_summary(soup):\n",
    "    job_summary_elements = soup.find('div', class_='box-summary')\n",
    "    sum_boxes = job_summary_elements.find_all('div', class_='summary-item')\n",
    "\n",
    "    test_dict = {}\n",
    "    for box in sum_boxes:\n",
    "        key = box.find('span', class_='content-label')\n",
    "        if key != None:\n",
    "            key = key.text.strip()\n",
    "            if key != 'Ngành Nghề':\n",
    "                val = box.find('span', class_='content')\n",
    "                val = val.text.strip()\n",
    "                test_dict[key]= val\n",
    "            else:\n",
    "                val_list = []\n",
    "                vals = box.find_all('a')\n",
    "                for val in vals:\n",
    "                    val_list.append([val.text.strip(), val.get('href')])\n",
    "                test_dict[key]= val_list\n",
    "                    \n",
    "\n",
    "    return test_dict\n",
    "\n",
    "   \n",
    "def save_to_df(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, \"html5lib\")\n",
    "\n",
    "\n",
    "    # JOB TITLE\n",
    "    job_title = soup.find('h1', class_='job-title').contents[0].strip()\n",
    "    \n",
    "    # LOCATION\n",
    "    location = soup.find('div', class_='location-name').text.strip()\n",
    "    location = location.split(':')[1].strip()\n",
    "    \n",
    "    # SALARY\n",
    "    job_salary = soup.find('span', class_='salary').text.strip()\n",
    "    \n",
    "    # EXPERIENCE\n",
    "    job_exp = None\n",
    "    \n",
    "    # COMPANY\n",
    "    org_name = soup.find('span', class_='name').text.strip()\n",
    "    \n",
    "    # COMPANY LINK\n",
    "    org_link = soup.find('div', class_='new-meta-company')\n",
    "    org_link = org_link.find('a').get('href')\n",
    "    \n",
    "    # DESCRIPTION\n",
    "    job_desc = soup.find('div', class_='description').text.strip()\n",
    "    \n",
    "    # BENEFITS\n",
    "    benefit_list = soup.find_all('div', class_='benefit-name')\n",
    "    job_benefits = []\n",
    "    for benefit in benefit_list:\n",
    "        job_benefits += [benefit.text.strip()]\n",
    "        \n",
    "    # REQUIRED SKILLS\n",
    "    job_skills = soup.find('div', class_='requirements').text.strip()\n",
    "    \n",
    "    # SUMMARY\n",
    "    job_summary = get_summary(soup)\n",
    "    \n",
    "    # CONTACT\n",
    "    contact_info = {'Contact': None, 'Location': None, 'Notes': None}\n",
    "    loc_div = soup.find('div', class_='job-locations')\n",
    "    contact_info['Location'] = loc_div.find('div', class_='location-name').text.strip()\n",
    "    \n",
    "    # KEYWORDS\n",
    "    keywords = get_keywords(soup)\n",
    "    \n",
    "    # POST DATE\n",
    "    job_post_date = job_summary['Ngày Đăng Tuyển']\n",
    "    \n",
    "    # LOGO URL\n",
    "    logo_url = soup.find('img', class_='logo').get('src')\n",
    "    \n",
    "    # FIELD\n",
    "    job_field = job_summary['Lĩnh vực'] \n",
    "    \n",
    "    job_data = {\n",
    "        'Job Title': job_title,\n",
    "        'Location': location, #City = location[-1]\n",
    "        'Salary': job_salary,\n",
    "        'Experience': job_exp,\n",
    "        'Company': org_name,\n",
    "        'Company Link': org_link,\n",
    "        'Description': job_desc,\n",
    "        'Benefits': job_benefits,\n",
    "        'Required Skills': job_skills,\n",
    "        'Job Summary': job_summary,\n",
    "        'Contact': contact_info,\n",
    "        'Keywords': keywords,\n",
    "        'Post date': job_post_date,\n",
    "        'Logo URL': logo_url,\n",
    "        'Field': job_field,\n",
    "        \n",
    "    }\n",
    "\n",
    "    jobs_data.append(job_data)\n",
    "    print(f'Finished {file_path}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2277c5",
   "metadata": {},
   "source": [
    "# Get links on job list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99570f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def scroll_to_end_and_back(driver):\n",
    "    # Maximize the window\n",
    "#     driver.maximize_window()\n",
    "    \n",
    "    # Zoom out to 25% (optional)\n",
    "    driver.execute_script(\"document.body.style.zoom = '25%'\")\n",
    "    for _ in range(5):\n",
    "        # Scroll down by 200px\n",
    "        driver.execute_script(\"window.scrollBy(0, 50);\")\n",
    "\n",
    "        # Wait for a short time (adjust as needed)\n",
    "        time.sleep(3)\n",
    "    \n",
    "    # Scroll down to the end of the page\n",
    "    while True:\n",
    "        # Get the current page height\n",
    "        current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # Scroll down to the bottom of the page\n",
    "        driver.execute_script(f\"window.scrollTo(0, {current_height});\")\n",
    "        \n",
    "        # Wait for a short time (adjust as needed)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Get the new page height after scrolling\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # If the new height is the same as the old height, you've reached the end\n",
    "        if new_height == current_height:\n",
    "            break\n",
    "    \n",
    "    # Scroll back to the start of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "def vnwork_get_job_links(vnwork_driver):\n",
    "    job_links = []\n",
    "    # Find all card elements on the page\n",
    "    card_elements = vnwork_driver.find_elements(By.CLASS_NAME, 'search_list')\n",
    "\n",
    "    for card_element in card_elements:\n",
    "        # Link to the job view\n",
    "        try:\n",
    "            link_element = card_element.find_element(By.CSS_SELECTOR, 'a')\n",
    "            link = link_element.get_attribute('href')\n",
    "            \n",
    "            if extract_and_save_ids(link) == True:\n",
    "                job_links.append(link)\n",
    "            else:\n",
    "                print('ALR EXISTED. Skipping....')\n",
    "                \n",
    "        except NoSuchElementException:\n",
    "            link = None\n",
    "    return job_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb827ed8",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59410ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_pages_file = vnwork_pages\n",
    "output_folder = r'E:\\Crawling Result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ffd28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_data = []\n",
    "errors = []\n",
    "all_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "404113ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall progress: 100.0%\n",
      "ALR EXISTED. Skipping....\n"
     ]
    }
   ],
   "source": [
    "# Open the file in append mode\n",
    "with open(website_pages_file, 'r') as f:\n",
    "    page_links = f.readlines()[:100]\n",
    "    \n",
    "vnwork_driver = webdriver.Chrome()\n",
    "page_count = 1\n",
    "\n",
    "output_file = 'VNWork_WantedLinks_Backup.txt'\n",
    "with open(output_file, 'a') as output_file:\n",
    "    for line in page_links:\n",
    "        page_url = line.strip()\n",
    "        page_progress = round((page_count * 100 / len(page_links)), 2)\n",
    "        print(f'Overall progress: {page_progress}%')\n",
    "\n",
    "        vnwork_driver.get(page_url)\n",
    "        numbered_page_path = saving_numbered_pages(page_url, output_folder)\n",
    "        scroll_to_end_and_back(vnwork_driver)\n",
    "\n",
    "        new_links = vnwork_get_job_links(vnwork_driver)\n",
    "        \n",
    "        backup_links = new_links\n",
    "        all_links += new_links\n",
    "\n",
    "        # Write the links to the output file\n",
    "        for link in new_links:\n",
    "            output_file.write(link + '\\n')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        page_count += 1\n",
    "\n",
    "        \n",
    "with open(\"VNWork_WantedLinks.txt\", \"w\") as link_file:\n",
    "    for link in all_links:\n",
    "        link_file.write(link + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da587310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall progress: 100.0%\n",
      "Finished E:\\Crawling Result\\vietnamworks\\2023\\11\\3\\2000\\[20231103 080240]_00001_truong-phong-quan-ly-chat-luong-may-mac.txt\n"
     ]
    }
   ],
   "source": [
    "jobs_data = []\n",
    "batch_size = 100\n",
    "start = 0\n",
    "\n",
    "with open(\"VNWork_WantedLinks.txt\", 'r') as f:\n",
    "    lines = f.readlines()[700:]\n",
    "    \n",
    "overall_sum = len(lines)\n",
    "overall_count = 1\n",
    "batch_number = 0\n",
    "    \n",
    "for hyperlink in lines:\n",
    "    overall_progress = round((overall_count / overall_sum)*100, 2)\n",
    "    print(f'Overall progress: {overall_progress}%')\n",
    "    \n",
    "    file_path = saving_detail_file(hyperlink, output_folder)\n",
    "    try:\n",
    "        save_to_df(file_path)\n",
    "        if len(jobs_data) % batch_size == 0:\n",
    "            batch_number = len(jobs_data) // batch_size\n",
    "            batch_df = pd.DataFrame(jobs_data[start : start+batch_size])\n",
    "            save_csv_file(batch_df, batch_number, 'vietnamworks', output_folder)\n",
    "            start += batch_size\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {file_path}: {str(e)}')\n",
    "        errors.append(file_path)\n",
    "\n",
    "        \n",
    "    overall_count += 1\n",
    "    clear_output(wait=True)\n",
    "\n",
    "# Save any remaining records to a final CSV file\n",
    "if len(jobs_data) % batch_size != 0:\n",
    "    saved_items_amt = batch_size * batch_number\n",
    "    batch_df = pd.DataFrame(jobs_data[saved_items_amt:])\n",
    "    save_csv_file(batch_df, batch_number + 1, 'vietnamworks', output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e619bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(jobs_data)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfecaae",
   "metadata": {},
   "source": [
    "## Re-iterate through all available html files as a whole (for re-running case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24051291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13207"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_folder = r'E:\\Crawling Result\\vietnamworks'  # Replace with your folder path\n",
    "\n",
    "# Function to list all subfolders recursively, excluding 'numbered pages'\n",
    "def list_subfolders(root_path):\n",
    "    subfolders = [x[0] for x in os.walk(root_path) if 'numbered pages' not in x[0]]\n",
    "    return subfolders\n",
    "\n",
    "def find_html_files(subfolders):\n",
    "    html_files = []\n",
    "    for subfolder in subfolders:\n",
    "        for file in os.listdir(subfolder):\n",
    "            if file.lower().endswith('.txt'):\n",
    "                html_files.append(os.path.join(subfolder, file))\n",
    "    return html_files\n",
    "\n",
    "subfolders = list_subfolders(root_folder)\n",
    "html_files = find_html_files(subfolders)\n",
    "\n",
    "len(html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94f012df",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'vietnamworks'\n",
    "website_pages_file = careerlink_pages\n",
    "output_folder = r'E:\\Crawling Result'\n",
    "\n",
    "jobs_data = []\n",
    "\n",
    "batch_size = 100\n",
    "start = 0\n",
    "overall_sum = len(html_files)\n",
    "overall_count = 1\n",
    "\n",
    "i = 400\n",
    "while True:\n",
    "    if i < len(html_files):\n",
    "        file_path = html_files[i]\n",
    "        \n",
    "        overall_progress = round((overall_count / overall_sum)*100, 2)\n",
    "        print(f'Overall progress: {overall_progress}%')\n",
    "\n",
    "        try:\n",
    "            save_to_df(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {file_path}: {str(e)}')\n",
    "            errors.append(file_path)\n",
    "\n",
    "            output_file = 'vietnamworks_Errors.txt'\n",
    "            with open(output_file, 'a') as output_file:\n",
    "                output_file.write(file_path + '\\n')\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    if len(jobs_data) % batch_size == 0:\n",
    "        batch_number = len(jobs_data) // batch_size\n",
    "        batch_df = pd.DataFrame(jobs_data[start : start+batch_size])\n",
    "        save_csv_file(batch_df, batch_number, domain, output_folder)\n",
    "        start += batch_size\n",
    "\n",
    "    overall_count += 1\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "# Save any remaining records to a final CSV file\n",
    "if len(jobs_data) % batch_size != 0:\n",
    "    saved_items_amt = batch_size * batch_number\n",
    "    batch_df = pd.DataFrame(jobs_data[saved_items_amt:])\n",
    "    save_csv_file(batch_df, batch_number + 1, domain, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ebbd529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13106, 15)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(jobs_data)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08600d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r'E:\\Crawling Result\\2023\\10\\19\\vietnamworks\\result'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    \n",
    "output_file = os.path.join(output_folder, 'merged_csv.csv')\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f4aa6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13106, 15)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vnwork_df = pd.read_csv(output_file)\n",
    "vnwork_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4fa98",
   "metadata": {},
   "source": [
    "# Hotel Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c582b",
   "metadata": {},
   "source": [
    "## Detail view extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2fdf838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overview(soup):\n",
    "    job_overview = soup.find('div', class_='overview-cotain')\n",
    "    overview_boxes = job_overview.find_all('div', class_='media-body')\n",
    "\n",
    "    overview_dict = {}\n",
    "\n",
    "    for box in overview_boxes:\n",
    "        key = box.find('strong').text.strip()\n",
    "        if key not in ('Loại hình', 'Ngành nghề', 'Cập nhật', 'Vị trí'):\n",
    "            val = box.find('br').next_sibling.strip()\n",
    "        elif key == 'Cập nhật':\n",
    "            val = box.find('span').text.strip()\n",
    "\n",
    "        elif key == 'Loại hình':\n",
    "            a_list = box.find_all('a')\n",
    "            val = []\n",
    "            for a in a_list:\n",
    "                val_list = [a.text.strip(), 'https://www.hoteljob.vn' + a.get('href')]\n",
    "                val += [val_list]\n",
    "        else:\n",
    "            val_list = [box.find('a').text.strip(), 'https://www.hoteljob.vn' + box.find('a').get('href')]\n",
    "            val = val_list  # Assign the list to val\n",
    "            if key == 'Vị trí':\n",
    "                key = 'Cấp Bậc'\n",
    "        overview_dict[key] = val\n",
    "\n",
    "    return overview_dict, overview_dict['Nơi làm việc'], overview_dict['Ngành nghề'], overview_dict['Cập nhật']\n",
    "\n",
    "def save_to_df(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html5lib\")\n",
    "\n",
    "    # TITLE\n",
    "    job_title = soup.find('h1').text.strip()\n",
    "\n",
    "    # DESCRIPTION, BENEFITS, SKILLS, CONTACT_INFO    \n",
    "    content_blocks = soup.find_all('div', class_='content-block')\n",
    "    content_block_list = []\n",
    "    for block in content_blocks:\n",
    "        content_block_list += [block.text.strip()]\n",
    "\n",
    "    job_desc = content_block_list[0]\n",
    "    job_benefits = content_block_list[1]\n",
    "    job_skills = content_block_list[2]\n",
    "    contact_info = content_block_list[3]\n",
    "\n",
    "    # LOCATION, FIELD, POST DAY\n",
    "    job_summary, location, job_field, job_post_date = get_overview(soup)\n",
    "\n",
    "\n",
    "    # LOGO URL\n",
    "    logo_url = soup.find('a', class_='avatar-item')\n",
    "    logo_url = 'https://www.hoteljob.vn' + logo_url.find('img').get('src')\n",
    "\n",
    "    # COMPANY NAME\n",
    "    org_box = soup.find('div', class_='ntd-right-info-item')\n",
    "    org_name = org_box.find('h2').text.strip()\n",
    "\n",
    "    # COMPANY LINK\n",
    "    org_link = 'https://www.hoteljob.vn' + org_box.find('a').get('href')\n",
    "    org_link\n",
    "\n",
    "    # SALARY\n",
    "    ul = soup.find('ul', class_=\"ul-hethan-mucluong\")\n",
    "    job_salary = ul.find_all('li')[1].find('strong').text.strip()\n",
    "\n",
    "    # XP, KW\n",
    "    job_exp, keywords = None, None\n",
    "    \n",
    "    job_data = {\n",
    "        'Job Title': job_title,\n",
    "        'Location': location, #City = location[-1]\n",
    "        'Salary': job_salary,\n",
    "        'Experience': job_exp,\n",
    "        'Company': org_name,\n",
    "        'Company Link': org_link,\n",
    "        'Description': job_desc,\n",
    "        'Benefits': job_benefits,\n",
    "        'Required Skills': job_skills,\n",
    "        'Job Summary': job_summary,\n",
    "        'Contact': contact_info,\n",
    "        'Keywords': keywords,\n",
    "        'Post date': job_post_date,\n",
    "        'Logo URL': logo_url,\n",
    "        'Field': job_field,\n",
    "        \n",
    "    }\n",
    "\n",
    "    jobs_data.append(job_data)\n",
    "    print(f'Finished {file_path}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cc0d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "jobs_data = []\n",
    "errors = []\n",
    "website_pages_file = hotel_pages\n",
    "output_folder = r'E:\\Crawling Result'\n",
    "# output_folder = r'E:\\Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "543ada88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall progress: 100.0%\n"
     ]
    }
   ],
   "source": [
    "jobs_data = []\n",
    "batch_size = 100\n",
    "start = 0\n",
    "with open(website_pages_file, 'r') as f:\n",
    "    lines = f.readlines()[:]\n",
    "    overall_sum = len(lines)\n",
    "    overall_count = 1\n",
    "    for line in lines:\n",
    "        page_url = line.strip()\n",
    "        numbered_page_path = saving_numbered_pages(page_url, output_folder)\n",
    "        \n",
    "        i = 0\n",
    "        overall_progress = round((overall_count / overall_sum)*100, 2)\n",
    "        print(f'Overall progress: {overall_progress}%')\n",
    "        while True:\n",
    "            \n",
    "            with open(numbered_page_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "            soup = BeautifulSoup(html_content, \"html5lib\")\n",
    "                \n",
    "            jobs = soup.find_all('a', class_='text-red')\n",
    "            links = []\n",
    "            \n",
    "            for job in jobs:\n",
    "                job_url = 'https://www.hoteljob.vn' + job.get('href')\n",
    "                links += [job_url]\n",
    "                \n",
    "            if i < len(links):\n",
    "                hyperlink = links[i]\n",
    "                \n",
    "                if extract_and_save_ids(hyperlink) == True:\n",
    "                    file_path = saving_detail_file(hyperlink, output_folder)\n",
    "                    \n",
    "                    try:\n",
    "                        save_to_df(file_path)\n",
    "                        \n",
    "                        if len(jobs_data) % batch_size == 0:\n",
    "                            batch_number = len(jobs_data) // batch_size\n",
    "                            batch_df = pd.DataFrame(jobs_data[start : start+batch_size])\n",
    "                            save_csv_file(batch_df, batch_number, 'hoteljob',output_folder)\n",
    "                            start += batch_size      \n",
    "                                              \n",
    "                    except Exception as e:\n",
    "                        print(f'Error processing {file_path}: {str(e)}')\n",
    "                        errors.append(file_path)\n",
    "                        \n",
    "                        output_file = 'HotelJob_Errors.txt'\n",
    "                        with open(output_file, 'a') as output_file:\n",
    "                            output_file.write(hyperlink + '\\n')\n",
    "                else:\n",
    "                    print('ALR EXISTED, skipping....')   \n",
    "                \n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "                \n",
    "        overall_count += 1\n",
    "        clear_output(wait=True)\n",
    "\n",
    "# Save any remaining records to a final CSV file\n",
    "if len(jobs_data) % batch_size != 0:\n",
    "    saved_items_amt = batch_size * batch_number\n",
    "    batch_df = pd.DataFrame(jobs_data[saved_items_amt:])\n",
    "    save_csv_file(batch_df, batch_number + 1, 'hoteljob', output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc3e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(jobs_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-iterate through all available html files as a whole (for re-running case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2408"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_folder = r'E:\\Crawling Result\\hoteljob'  # Replace with your folder path\n",
    "\n",
    "# Function to list all subfolders recursively, excluding 'numbered pages'\n",
    "def list_subfolders(root_path):\n",
    "    subfolders = [x[0] for x in os.walk(root_path) if 'numbered pages' not in x[0]]\n",
    "    return subfolders\n",
    "\n",
    "def find_html_files(subfolders):\n",
    "    html_files = []\n",
    "    for subfolder in subfolders:\n",
    "        for file in os.listdir(subfolder):\n",
    "            if file.lower().endswith('.txt'):\n",
    "                html_files.append(os.path.join(subfolder, file))\n",
    "    return html_files\n",
    "\n",
    "subfolders = list_subfolders(root_folder)\n",
    "html_files = find_html_files(subfolders)\n",
    "\n",
    "len(html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall progress: 100.0%\n",
      "Error processing E:\\Crawling Result\\hoteljob\\2023\\9\\30\\1000\\[20230930 103516]_00001_122402-duty-manager.txt: list index out of range\n"
     ]
    }
   ],
   "source": [
    "domain = 'hoteljob'\n",
    "website_pages_file = careerlink_pages\n",
    "output_folder = r'E:\\Crawling Result'\n",
    "errors = []\n",
    "\n",
    "jobs_data = []\n",
    "\n",
    "batch_size = 100\n",
    "start = 0\n",
    "overall_sum = len(html_files)\n",
    "overall_count = 1\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    if i < len(html_files):\n",
    "        file_path = html_files[i]\n",
    "        \n",
    "        overall_progress = round((overall_count / overall_sum)*100, 2)\n",
    "        print(f'Overall progress: {overall_progress}%')\n",
    "\n",
    "        try:\n",
    "            save_to_df(file_path)\n",
    "            if len(jobs_data) % batch_size == 0:\n",
    "                batch_number = len(jobs_data) // batch_size\n",
    "                batch_df = pd.DataFrame(jobs_data[start : start+batch_size])\n",
    "                save_csv_file(batch_df, batch_number, domain, output_folder)\n",
    "                start += batch_size    \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f'Error processing {file_path}: {str(e)}')\n",
    "            errors.append(file_path)\n",
    "\n",
    "            output_file = 'HotelJob_Errors.txt'\n",
    "            with open(output_file, 'a') as output_file:\n",
    "                output_file.write(file_path + '\\n')\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    overall_count += 1\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "# Save any remaining records to a final CSV file\n",
    "if len(jobs_data) % batch_size != 0:\n",
    "    saved_items_amt = batch_size * batch_number\n",
    "    batch_df = pd.DataFrame(jobs_data[saved_items_amt:])\n",
    "    save_csv_file(batch_df, batch_number + 1, domain, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(jobs_data)\n",
    "print(df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
